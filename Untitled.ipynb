{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b02da60-477a-45ac-88bc-273d7642e1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import os\n",
    "from dask import delayed, compute\n",
    "from functools import reduce\n",
    "from scipy.interpolate import griddata\n",
    "import dask\n",
    "import itertools\n",
    "from line_profiler import LineProfiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4889da57-2c0a-4531-924a-9f0525ed5d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_function():\n",
    "    # Load necessary data\n",
    "    convcodes = pd.read_csv(\"/storage/homefs/ch21o450/scripts/BioScenComb/data/IUCN_LUH_converion_table_Carlson.csv\")\n",
    "    time=[35]\n",
    "\n",
    "    taxas = [\"Amphibians\"]\n",
    "    models =[\"GAM\",\"GBM\"] \n",
    "\n",
    "    years= ['1845', '1990', '1995', '2009', '2010', '2020', '2026', '2032', '2048', '2050','2052', '2056', '2080', '2100', '2150', '2200', '2250']\n",
    "    year_indices = {35: 9, 65: 12, 85: 13}\n",
    "    selected_year = years[year_indices[time[0]]]\n",
    "    if time[0] == 35 or time[0] == 65:\n",
    "        model_names = ['GFDL-ESM2M', 'IPSL-CM5A-LR', 'HadGEM2-ES', 'MIROC5']\n",
    "        bioscen_model_names = ['GFDL.ESM2M', 'IPSL.CM5A-LR', 'HadGEM2.ES', 'MIROC5']\n",
    "    elif time[0] == 85:\n",
    "        model_names = ['IPSL-CM5A-LR', 'HadGEM2-ES', 'MIROC5']\n",
    "        bioscen_model_names = ['IPSL.CM5A-LR', 'HadGEM2.ES', 'MIROC5']\n",
    "\n",
    "    scenarios = [\"rcp26\",\"rcp60\"]\n",
    "    ssprcps_shorts = [\"ssp126\",\"ssp460\"]\n",
    "    combinations = list(itertools.product(models, model_names))\n",
    "\n",
    "    for taxa in taxas:# Get all possible combinations of models and model_names    \n",
    "        for model in models :\n",
    "            for model_name in model_names:\n",
    "                for bioscen_model_name in bioscen_model_names:\n",
    "                    for scenario in scenarios:\n",
    "                        for l, ssprcp_short in enumerate(ssprcps_shorts):\n",
    "\n",
    "                            convcodes = pd.read_csv(\"/storage/homefs/ch21o450/scripts/BioScenComb/data/IUCN_LUH_converion_table_Carlson.csv\")\n",
    "                            dir_habclass = \"/storage/homefs/ch21o450/IUCN/Habitat_Classifications/\" + taxa + \"/\"\n",
    "\n",
    "                            dir_species = \"/storage/workspaces/wa_climate/climate_trt/data/BioScen15/individual_projections/\" + taxa+ \"_\" + model +\"_results_climate/\"\n",
    "                            available_file = os.listdir(dir_species)\n",
    "                            available_names = [x.split(\".csv\")[0] for x in available_file]\n",
    "\n",
    "                            formatted_names = []\n",
    "\n",
    "                            for species_name in available_names:\n",
    "                                split_species_name = species_name.split(\"_\")[:2]\n",
    "                                formatted_species_name = \" \".join(split_species_name)\n",
    "                                formatted_names.append(formatted_species_name)\n",
    "\n",
    "                            results = []\n",
    "                            for i, species_name in enumerate(formatted_names[:2]):\n",
    "                                formatted_species_name = species_name.replace(\" \", \"_\")\n",
    "\n",
    "                                for file_name in available_file:\n",
    "                                    if formatted_species_name in file_name and model + '_dispersal.csv.xz' in file_name:\n",
    "                                        species_file = file_name\n",
    "                                        species_file2 = [x.split(\".csv\")[0] for x in species_file] \n",
    "                                        break\n",
    "                                else:\n",
    "                                    bioscen_species = None\n",
    "                                    continue\n",
    "\n",
    "                                bioscen_species = pd.read_csv(dir_species + file_name)\n",
    "\n",
    "                                available_files_iucn = formatted_species_name + \".csv\"\n",
    "                                if available_files_iucn in os.listdir(dir_habclass):\n",
    "                                    IUCN = pd.read_csv(dir_habclass + available_files_iucn)\n",
    "                                else:\n",
    "                                    continue\n",
    "\n",
    "                                lon = bioscen_species[\"x\"]\n",
    "                                lat = bioscen_species[\"y\"]\n",
    "                                z = bioscen_species[bioscen_model_name + '_' + scenario + '_' + selected_year]\n",
    "\n",
    "                                df = pd.DataFrame({\"lon\": lon, \"lat\": lat, \"vals\": z})\n",
    "                                df = df.fillna(0)\n",
    "                                convcodes_renamed = convcodes.rename(columns={'IUCN_hab':'result.code'})\n",
    "                                Habitats = IUCN.merge(convcodes_renamed, left_on='result.code', right_on='result.code')\n",
    "\n",
    "                                keys = ['LUH1', 'LUH2', 'LUH3', 'LUH4', 'LUH5', 'LUH6', 'LUH7', 'LUH8','LUH9','LUH10', 'LUH11', 'LUH12']\n",
    "                                split_cols = Habitats['LUH'].str.split('.', expand=True)\n",
    "                                for i, key in enumerate(keys):\n",
    "                                    if i < len(split_cols.columns):\n",
    "                                        Habitats[key] = split_cols[i]\n",
    "                                    else:\n",
    "                                        Habitats[key] = pd.Series(dtype='float64')\n",
    "                                if len(Habitats.columns) > len(keys) + 1:\n",
    "                                    num_missing_cols = len(Habitats.columns) - len(keys) - 1\n",
    "                                    Habitats = Habitats.reindex(columns=list(Habitats.columns) + ['LUH{}'.format(i) for i in range(13, 13 + num_missing_cols)], fill_value=np.nan)\n",
    "                                    Habitats.drop('LUH', axis=1, inplace=True)\n",
    "                                Habitats_suitable = Habitats[Habitats['result.suitability'] == 'Suitable'].copy()\n",
    "\n",
    "                                LandUseList = \"/storage/workspaces/wa_climate/climate_trt/chari/LUH2/remapped_luh2_\" + ssprcps_shorts[l] + \".nc\"\n",
    "\n",
    "                                #isimip = xr.open_dataarray(\"/storage/workspaces/wa_climate/climate_trt/data/ISIMIP/ISIMIP3b/InputData/GCM/global/miroc6_r1i1p1f1_w5e5_ssp585_tasmin_global_daily_2071_2080.nc\")\n",
    "\n",
    "\n",
    "                                ncfname = LandUseList\n",
    "                                da_landuse =  xr.open_dataset(ncfname, decode_times=False)\n",
    "                                da_landuse = da_landuse.isel(time=time)\n",
    "\n",
    "                                #prifdf_bin = xr.where(prifdf > 0, 1, 0)\n",
    "                                df_sdm =df\n",
    "\n",
    "                                #build an empty np.array \n",
    "                                np_empty = np.zeros_like(da_landuse['primf'].values, dtype=float)\n",
    "\n",
    "                                #isimip_lats = isimip['lat'].values\n",
    "                                #isimip_lons = isimip['lon'].values\n",
    "\n",
    "                                lats = da_landuse['lat'].values\n",
    "                                lons = da_landuse['lon'].values\n",
    "\n",
    "                                da_empty = xr.DataArray(np_empty, coords=[time, lats, lons], dims=['time','lats','lons'])\n",
    "                                da_landclim = da_empty.assign_attrs(da_landuse)\n",
    "\n",
    "                                keys = [\"primn\" if row[f\"LUH{i}\"] == \"primn\" else row[f\"LUH{i}\"] for _, row in Habitats_suitable.iterrows() for i in range(1, 5) if pd.notna(row[f\"LUH{i}\"])]\n",
    "                                keys = list(set(keys))\n",
    "\n",
    "                                # Compute the product with the \"newvalue\" column and assign it to a new column in the merged DataFrame\n",
    "\n",
    "                                # Compute the product with the \"newvalue\" column and assign it to a new column in the merged DataFrame\n",
    "                                latitudes = df_sdm['lat'].unique()\n",
    "                                longitudes = df_sdm['lon'].unique()\n",
    "\n",
    "                                lats_sorted = np.sort(latitudes)\n",
    "                                lons_sorted = np.sort(longitudes)\n",
    "\n",
    "                               # Create a dictionary with (lat, lon) tuples as keys and the corresponding values from df_sdm as values\n",
    "                                sdm_dict = {(lat, lon): vals for lat, lon, vals in df_sdm[['lat', 'lon', 'vals']].to_numpy()}\n",
    "\n",
    "                                # Initialize the newvalue_array with NaNs instead of zeros\n",
    "                                newvalue_array = np.full((len(lats_sorted), len(lons_sorted)), np.nan)\n",
    "\n",
    "                                # Loop over the latitudes and longitudes and use the dictionary to perform the lookup\n",
    "                                for i, lat in enumerate(lats_sorted):\n",
    "                                    for j, lon in enumerate(lons_sorted):\n",
    "                                        vals = sdm_dict.get((lat, lon), np.nan)\n",
    "                                        if not np.isnan(vals):\n",
    "                                            newvalue_array[i, j] = vals\n",
    "\n",
    "\n",
    "                                da = xr.DataArray(newvalue_array, coords=[lats_sorted, lons_sorted], dims=['lat', 'lon'])\n",
    "                                # Interpolate the values of newvalue to the dimensions of A\n",
    "                                interpolated_values = da.interp(lat=lats, lon=lons)\n",
    "\n",
    "                                # Add the interpolated values to the A DataArray\n",
    "                                da_landuse['newvalue'] = interpolated_values\n",
    "                                da_landuse['newvalue'] = interpolated_values.fillna(0)\n",
    "                                for code in keys: \n",
    "                                    # Compute the product with the LUH code and the \"newvalue\" column, and assign it to a new column in the merged DataFrame\n",
    "                                    np_empty = np.zeros_like(da_landuse[code].values, dtype=float)\n",
    "                                    da_landuse[f\"{code}_bin\"] = da_landuse[code] * da_landuse[\"newvalue\"]\n",
    "\n",
    "                                    # Select the DataArrays ending in \"_bin\"\n",
    "                                    bin_arrays = [da_landuse[var] for var in da_landuse.data_vars if var.endswith(\"_bin\")]\n",
    "\n",
    "                                    # Multiply all the arrays together\n",
    "                                    sum_bin = reduce(lambda x, y: x + y, bin_arrays)\n",
    "                                    # Assign the \"product_bin\" attribute to the da_landuse DataArray\n",
    "                                    da_landuse[\"sum_bin\"] = sum_bin\n",
    "                                    difference = da_landuse[\"sum_bin\"] - da_landuse[\"newvalue\"]\n",
    "                                    da_landuse[\"difference_filter\"] = difference\n",
    "\n",
    "                                    da_landclim = da_landclim.assign_attrs(da_landuse)\n",
    "\n",
    "                                    da_landuse.to_netcdf(\"/storage/homefs/ch21o450/scripts/BioScenComb/data/LandClim_Output/\" + model+ \"/\" + taxa + \"/\" + model_name + \"/\" + scenario + \"/\" + formatted_species_name + \"_\" + str(time)+ \".nc\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5399e04-5ad8-4816-ac7a-b4debea043c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-09 s\n",
      "\n",
      "Total time: 214.139 s\n",
      "File: /scratch/local/49936449/ipykernel_16338/3844084110.py\n",
      "Function: my_function at line 1\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     1                                           def my_function():\n",
      "     2                                               # Load necessary data\n",
      "     3         1    2787914.0 2787914.0      0.0      convcodes = pd.read_csv(\"/storage/homefs/ch21o450/scripts/BioScenComb/data/IUCN_LUH_converion_table_Carlson.csv\")\n",
      "     4         1        280.0    280.0      0.0      time=[35]\n",
      "     5                                           \n",
      "     6         1        170.0    170.0      0.0      taxas = [\"Amphibians\"]\n",
      "     7         1        220.0    220.0      0.0      models =[\"GAM\",\"GBM\"] \n",
      "     8                                           \n",
      "     9         1        860.0    860.0      0.0      years= ['1845', '1990', '1995', '2009', '2010', '2020', '2026', '2032', '2048', '2050','2052', '2056', '2080', '2100', '2150', '2200', '2250']\n",
      "    10         1        540.0    540.0      0.0      year_indices = {35: 9, 65: 12, 85: 13}\n",
      "    11         1        410.0    410.0      0.0      selected_year = years[year_indices[time[0]]]\n",
      "    12         1        320.0    320.0      0.0      if time[0] == 35 or time[0] == 65:\n",
      "    13         1        340.0    340.0      0.0          model_names = ['GFDL-ESM2M', 'IPSL-CM5A-LR', 'HadGEM2-ES', 'MIROC5']\n",
      "    14         1        450.0    450.0      0.0          bioscen_model_names = ['GFDL.ESM2M', 'IPSL.CM5A-LR', 'HadGEM2.ES', 'MIROC5']\n",
      "    15                                               elif time[0] == 85:\n",
      "    16                                                   model_names = ['IPSL-CM5A-LR', 'HadGEM2-ES', 'MIROC5']\n",
      "    17                                                   bioscen_model_names = ['IPSL.CM5A-LR', 'HadGEM2.ES', 'MIROC5']\n",
      "    18                                           \n",
      "    19         1        420.0    420.0      0.0      scenarios = [\"rcp26\",\"rcp60\"]\n",
      "    20         1        290.0    290.0      0.0      ssprcps_shorts = [\"ssp126\",\"ssp460\"]\n",
      "    21         1       3960.0   3960.0      0.0      combinations = list(itertools.product(models, model_names))\n",
      "    22                                           \n",
      "    23         1        610.0    610.0      0.0      for taxa in taxas:# Get all possible combinations of models and model_names    \n",
      "    24         2      11640.0   5820.0      0.0          for model in models :\n",
      "    25         8       9132.0   1141.5      0.0              for model_name in model_names:\n",
      "    26        32      28920.0    903.8      0.0                  for bioscen_model_name in bioscen_model_names:\n",
      "    27        64      41084.0    641.9      0.0                      for scenario in scenarios:\n",
      "    28       128     221253.0   1728.5      0.0                          for l, ssprcp_short in enumerate(ssprcps_shorts):\n",
      "    29                                           \n",
      "    30       128  293728170.0 2294751.3      0.1                              convcodes = pd.read_csv(\"/storage/homefs/ch21o450/scripts/BioScenComb/data/IUCN_LUH_converion_table_Carlson.csv\")\n",
      "    31       128     229811.0   1795.4      0.0                              dir_habclass = \"/storage/homefs/ch21o450/IUCN/Habitat_Classifications/\" + taxa + \"/\"\n",
      "    32                                           \n",
      "    33       128     215264.0   1681.8      0.0                              dir_species = \"/storage/workspaces/wa_climate/climate_trt/data/BioScen15/individual_projections/\" + taxa+ \"_\" + model +\"_results_climate/\"\n",
      "    34       128  385281429.0 3010011.2      0.2                              available_file = os.listdir(dir_species)\n",
      "    35       128  121113532.0 946199.5      0.1                              available_names = [x.split(\".csv\")[0] for x in available_file]\n",
      "    36                                           \n",
      "    37       128   13402506.0 104707.1      0.0                              formatted_names = []\n",
      "    38                                           \n",
      "    39    379392   75081764.0    197.9      0.0                              for species_name in available_names:\n",
      "    40    379392  230052314.0    606.4      0.1                                  split_species_name = species_name.split(\"_\")[:2]\n",
      "    41    379392  144060478.0    379.7      0.1                                  formatted_species_name = \" \".join(split_species_name)\n",
      "    42    379392  123054065.0    324.3      0.1                                  formatted_names.append(formatted_species_name)\n",
      "    43                                           \n",
      "    44       128      95090.0    742.9      0.0                              results = []\n",
      "    45       256    1215653.0   4748.6      0.0                              for i, species_name in enumerate(formatted_names[:2]):\n",
      "    46       256    1307845.0   5108.8      0.0                                  formatted_species_name = species_name.replace(\" \", \"_\")\n",
      "    47                                           \n",
      "    48       384     311033.0    810.0      0.0                                  for file_name in available_file:\n",
      "    49       256     715434.0   2794.7      0.0                                      if formatted_species_name in file_name and model + '_dispersal.csv.xz' in file_name:\n",
      "    50       256     121937.0    476.3      0.0                                          species_file = file_name\n",
      "    51       256    5380003.0  21015.6      0.0                                          species_file2 = [x.split(\".csv\")[0] for x in species_file] \n",
      "    52       256     133377.0    521.0      0.0                                          break\n",
      "    53                                                                           else:\n",
      "    54                                                                               bioscen_species = None\n",
      "    55                                                                               continue\n",
      "    56                                           \n",
      "    57       256 50086239266.0 195649372.1     23.4                                  bioscen_species = pd.read_csv(dir_species + file_name)\n",
      "    58                                           \n",
      "    59       256     571131.0   2231.0      0.0                                  available_files_iucn = formatted_species_name + \".csv\"\n",
      "    60       192  364645094.0 1899193.2      0.2                                  if available_files_iucn in os.listdir(dir_habclass):\n",
      "    61       192  537148251.0 2797647.1      0.3                                      IUCN = pd.read_csv(dir_habclass + available_files_iucn)\n",
      "    62                                                                           else:\n",
      "    63                                                                               continue\n",
      "    64                                           \n",
      "    65       192   33176857.0 172796.1      0.0                                  lon = bioscen_species[\"x\"]\n",
      "    66       192    8432143.0  43917.4      0.0                                  lat = bioscen_species[\"y\"]\n",
      "    67       192   31483308.0 163975.6      0.0                                  z = bioscen_species[bioscen_model_name + '_' + scenario + '_' + selected_year]\n",
      "    68                                           \n",
      "    69       192   91745685.0 477842.1      0.0                                  df = pd.DataFrame({\"lon\": lon, \"lat\": lat, \"vals\": z})\n",
      "    70       192   29142055.0 151781.5      0.0                                  df = df.fillna(0)\n",
      "    71       192   85138522.0 443429.8      0.0                                  convcodes_renamed = convcodes.rename(columns={'IUCN_hab':'result.code'})\n",
      "    72       192  618258451.0 3220096.1      0.3                                  Habitats = IUCN.merge(convcodes_renamed, left_on='result.code', right_on='result.code')\n",
      "    73                                           \n",
      "    74       192     326097.0   1698.4      0.0                                  keys = ['LUH1', 'LUH2', 'LUH3', 'LUH4', 'LUH5', 'LUH6', 'LUH7', 'LUH8','LUH9','LUH10', 'LUH11', 'LUH12']\n",
      "    75       192  144536432.0 752793.9      0.1                                  split_cols = Habitats['LUH'].str.split('.', expand=True)\n",
      "    76      2304    1644142.0    713.6      0.0                                  for i, key in enumerate(keys):\n",
      "    77      1920    2714682.0   1413.9      0.0                                      if i < len(split_cols.columns):\n",
      "    78       384  174713607.0 454983.4      0.1                                          Habitats[key] = split_cols[i]\n",
      "    79                                                                               else:\n",
      "    80      1920 1933858817.0 1007218.1      0.9                                          Habitats[key] = pd.Series(dtype='float64')\n",
      "    81       192     256757.0   1337.3      0.0                                  if len(Habitats.columns) > len(keys) + 1:\n",
      "    82       192     162148.0    844.5      0.0                                      num_missing_cols = len(Habitats.columns) - len(keys) - 1\n",
      "    83       192  229810840.0 1196931.5      0.1                                      Habitats = Habitats.reindex(columns=list(Habitats.columns) + ['LUH{}'.format(i) for i in range(13, 13 + num_missing_cols)], fill_value=np.nan)\n",
      "    84       192  160989491.0 838486.9      0.1                                      Habitats.drop('LUH', axis=1, inplace=True)\n",
      "    85       192  144982195.0 755115.6      0.1                                  Habitats_suitable = Habitats[Habitats['result.suitability'] == 'Suitable'].copy()\n",
      "    86                                           \n",
      "    87       192     293360.0   1527.9      0.0                                  LandUseList = \"/storage/workspaces/wa_climate/climate_trt/chari/LUH2/remapped_luh2_\" + ssprcps_shorts[l] + \".nc\"\n",
      "    88                                           \n",
      "    89                                                                           #isimip = xr.open_dataarray(\"/storage/workspaces/wa_climate/climate_trt/data/ISIMIP/ISIMIP3b/InputData/GCM/global/miroc6_r1i1p1f1_w5e5_ssp585_tasmin_global_daily_2071_2080.nc\")\n",
      "    90                                           \n",
      "    91                                           \n",
      "    92       192      51195.0    266.6      0.0                                  ncfname = LandUseList\n",
      "    93       192 3721798461.0 19384367.0      1.7                                  da_landuse =  xr.open_dataset(ncfname, decode_times=False)\n",
      "    94       192  454816190.0 2368834.3      0.2                                  da_landuse = da_landuse.isel(time=time)\n",
      "    95                                           \n",
      "    96                                                                           #prifdf_bin = xr.where(prifdf > 0, 1, 0)\n",
      "    97       192    3160065.0  16458.7      0.0                                  df_sdm =df\n",
      "    98                                           \n",
      "    99                                                                           #build an empty np.array \n",
      "   100       192  515398970.0 2684369.6      0.2                                  np_empty = np.zeros_like(da_landuse['primf'].values, dtype=float)\n",
      "   101                                           \n",
      "   102                                                                           #isimip_lats = isimip['lat'].values\n",
      "   103                                                                           #isimip_lons = isimip['lon'].values\n",
      "   104                                           \n",
      "   105       192   12503618.0  65123.0      0.0                                  lats = da_landuse['lat'].values\n",
      "   106       192    5473588.0  28508.3      0.0                                  lons = da_landuse['lon'].values\n",
      "   107                                           \n",
      "   108       192  113451417.0 590892.8      0.1                                  da_empty = xr.DataArray(np_empty, coords=[time, lats, lons], dims=['time','lats','lons'])\n",
      "   109       192  176749632.0 920571.0      0.1                                  da_landclim = da_empty.assign_attrs(da_landuse)\n",
      "   110                                           \n",
      "   111       192  222245504.0 1157528.7      0.1                                  keys = [\"primn\" if row[f\"LUH{i}\"] == \"primn\" else row[f\"LUH{i}\"] for _, row in Habitats_suitable.iterrows() for i in range(1, 5) if pd.notna(row[f\"LUH{i}\"])]\n",
      "   112       192     526493.0   2742.2      0.0                                  keys = list(set(keys))\n",
      "   113                                           \n",
      "   114                                                                           # Compute the product with the \"newvalue\" column and assign it to a new column in the merged DataFrame\n",
      "   115                                           \n",
      "   116                                                                           # Compute the product with the \"newvalue\" column and assign it to a new column in the merged DataFrame\n",
      "   117       192   62482337.0 325428.8      0.0                                  latitudes = df_sdm['lat'].unique()\n",
      "   118       192   36508483.0 190148.3      0.0                                  longitudes = df_sdm['lon'].unique()\n",
      "   119                                           \n",
      "   120       192    3774728.0  19660.0      0.0                                  lats_sorted = np.sort(latitudes)\n",
      "   121       192    1462143.0   7615.3      0.0                                  lons_sorted = np.sort(longitudes)\n",
      "   122                                           \n",
      "   123                                                                          # Create a dictionary with (lat, lon) tuples as keys and the corresponding values from df_sdm as values\n",
      "   124       192 2413464926.0 12570129.8      1.1                                  sdm_dict = {(lat, lon): vals for lat, lon, vals in df_sdm[['lat', 'lon', 'vals']].to_numpy()}\n",
      "   125                                           \n",
      "   126                                                                           # Initialize the newvalue_array with NaNs instead of zeros\n",
      "   127       192   13053798.0  67988.5      0.0                                  newvalue_array = np.full((len(lats_sorted), len(lons_sorted)), np.nan)\n",
      "   128                                           \n",
      "   129                                                                           # Loop over the latitudes and longitudes and use the dictionary to perform the lookup\n",
      "   130     21376    9738115.0    455.6      0.0                                  for i, lat in enumerate(lats_sorted):\n",
      "   131   3612672 1444653366.0    399.9      0.7                                      for j, lon in enumerate(lons_sorted):\n",
      "   132   3612672 2328376441.0    644.5      1.1                                          vals = sdm_dict.get((lat, lon), np.nan)\n",
      "   133   2224064 3876352773.0   1742.9      1.8                                          if not np.isnan(vals):\n",
      "   134   1388608  730292541.0    525.9      0.3                                              newvalue_array[i, j] = vals\n",
      "   135                                           \n",
      "   136                                           \n",
      "   137       192  112783078.0 587411.9      0.1                                  da = xr.DataArray(newvalue_array, coords=[lats_sorted, lons_sorted], dims=['lat', 'lon'])\n",
      "   138                                                                           # Interpolate the values of newvalue to the dimensions of A\n",
      "   139       192 1178910365.0 6140158.2      0.6                                  interpolated_values = da.interp(lat=lats, lon=lons)\n",
      "   140                                           \n",
      "   141                                                                           # Add the interpolated values to the A DataArray\n",
      "   142       192  470620634.0 2451149.1      0.2                                  da_landuse['newvalue'] = interpolated_values\n",
      "   143       192  756017682.0 3937592.1      0.4                                  da_landuse['newvalue'] = interpolated_values.fillna(0)\n",
      "   144       768    1259954.0   1640.6      0.0                                  for code in keys: \n",
      "   145                                                                               # Compute the product with the LUH code and the \"newvalue\" column, and assign it to a new column in the merged DataFrame\n",
      "   146       768 1422700809.0 1852475.0      0.7                                      np_empty = np.zeros_like(da_landuse[code].values, dtype=float)\n",
      "   147       768 3250515790.0 4232442.4      1.5                                      da_landuse[f\"{code}_bin\"] = da_landuse[code] * da_landuse[\"newvalue\"]\n",
      "   148                                           \n",
      "   149                                                                               # Select the DataArrays ending in \"_bin\"\n",
      "   150       768   75152190.0  97854.4      0.0                                      bin_arrays = [da_landuse[var] for var in da_landuse.data_vars if var.endswith(\"_bin\")]\n",
      "   151                                           \n",
      "   152                                                                               # Multiply all the arrays together\n",
      "   153       768 2010858261.0 2618305.0      0.9                                      sum_bin = reduce(lambda x, y: x + y, bin_arrays)\n",
      "   154                                                                               # Assign the \"product_bin\" attribute to the da_landuse DataArray\n",
      "   155       768 1978666768.0 2576389.0      0.9                                      da_landuse[\"sum_bin\"] = sum_bin\n",
      "   156       768 1045538415.0 1361378.1      0.5                                      difference = da_landuse[\"sum_bin\"] - da_landuse[\"newvalue\"]\n",
      "   157       768 1973649903.0 2569856.6      0.9                                      da_landuse[\"difference_filter\"] = difference\n",
      "   158                                           \n",
      "   159       768  455388129.0 592953.3      0.2                                      da_landclim = da_landclim.assign_attrs(da_landuse)\n",
      "   160                                           \n",
      "   161       768 127182136363.0 165601740.1     59.4                                      da_landuse.to_netcdf(\"/storage/homefs/ch21o450/scripts/BioScenComb/data/LandClim_Output/\" + model+ \"/\" + taxa + \"/\" + model_name + \"/\" + scenario + \"/\" + formatted_species_name + \"_\" + str(time)+ \".nc\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "profiler = LineProfiler()\n",
    "profiler.add_function(my_function)\n",
    "profiler.run('my_function()')\n",
    "profiler.print_stats()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
