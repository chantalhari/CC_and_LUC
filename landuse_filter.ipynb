{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a2ad856-f03e-4636-9df6-2416e62edd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "#pip install  rioxarray==0.3.1\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import rioxarray\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import cartopy.crs as ccrs\n",
    "import rasterio\n",
    "import os\n",
    "import matplotlib.colors\n",
    "scriptsdir = os.getcwd()\n",
    "from scipy.interpolate import griddata\n",
    "from functools import reduce\n",
    "import xarray\n",
    "import itertools\n",
    "import argparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262254b1-ce7a-446e-9dc8-27475ddf8456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *************************************************\n",
    "# Get the command line arguments\n",
    "# *************************************************\n",
    "ap = argparse.ArgumentParser()\n",
    "\n",
    "# collect the function arguments\n",
    "ap.add_argument('-t', '--time', type=int, help=\"time, integer\", nargs=\"+\", required=True)\n",
    "ap.add_argument('-m', '--model', type=str, help=\"model, string\", nargs=\"+\", required=True)\n",
    "ap.add_argument('-a', '--taxa', type=str, help=\"taxa, string\", nargs=\"+\", required=True)\n",
    "\n",
    "# parse the arguments to the args object\n",
    "args = ap.parse_args()\n",
    "\n",
    "# *************************************************\n",
    "# Get arguments\n",
    "# *************************************************\n",
    "print(args)\n",
    "\n",
    "time = args.time\n",
    "models = args.model\n",
    "taxas = args.taxa\n",
    "\n",
    "\n",
    "taxas = [\"Amphibians\"]\n",
    "models =[\"GAM\",\"GBM\"] \n",
    "\n",
    "years= ['1845', '1990', '1995', '2009', '2010', '2020', '2026', '2032', '2048', '2050','2052', '2056', '2080', '2100', '2150', '2200', '2250']\n",
    "year_indices = {35: 9, 65: 12, 85: 13}\n",
    "selected_year = years[year_indices[time[0]]]\n",
    "if time[0] == 35 or time[0] == 65:\n",
    "    model_names = ['GFDL-ESM2M', 'IPSL-CM5A-LR', 'HadGEM2-ES', 'MIROC5']\n",
    "    bioscen_model_names = ['GFDL.ESM2M', 'IPSL.CM5A-LR', 'HadGEM2.ES', 'MIROC5']\n",
    "elif time[0] == 85:\n",
    "    model_names = ['IPSL-CM5A-LR', 'HadGEM2-ES', 'MIROC5']\n",
    "    bioscen_model_names = ['IPSL.CM5A-LR', 'HadGEM2.ES', 'MIROC5']\n",
    "    \n",
    "scenarios = [\"rcp26\",\"rcp60\"]\n",
    "ssprcps_shorts = [\"ssp126\",\"ssp460\"]\n",
    "ssprcps_longs = [\"ssp1_rcp2.6\",\"ssp4_rcp6.0\"]\n",
    "combinations = list(itertools.product(models, model_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f73069ce-17a9-44de-8edd-2d53e2e1daac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed9d0c7-9bdf-4351-957d-950b8b505f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "for taxa in taxas:# Get all possible combinations of models and model_names    \n",
    "    for model in models :\n",
    "        for model_name in model_names:\n",
    "            for bioscen_model_name in bioscen_model_names:\n",
    "                for scenario in scenarios:\n",
    "                    for k, ssprcp_long in enumerate(ssprcps_longs):\n",
    "                        for l, ssprcp_short in enumerate(ssprcps_shorts):\n",
    "\n",
    "\n",
    "                            convcodes = pd.read_csv(\"/storage/homefs/ch21o450/scripts/BioScenComb/data/IUCN_LUH_converion_table_Carlson.csv\")\n",
    "                            dir_habclass = \"/storage/homefs/ch21o450/IUCN/Habitat_Classifications/\" + taxa + \"/\"\n",
    "\n",
    "                            dir_species = \"/storage/workspaces/wa_climate/climate_trt/data/BioScen15/individual_projections/\" + taxa+ \"_\" + model +\"_results_climate/\"\n",
    "                            available_file = os.listdir(dir_species)\n",
    "                            available_names = [x.split(\".csv\")[0] for x in available_file]\n",
    "\n",
    "                            formatted_names = []\n",
    "\n",
    "                            for species_name in available_names:\n",
    "                                split_species_name = species_name.split(\"_\")[:2]\n",
    "                                formatted_species_name = \" \".join(split_species_name)\n",
    "                                formatted_names.append(formatted_species_name)\n",
    "\n",
    "                            results = []\n",
    "                            for i, species_name in enumerate(formatted_names[:2]):\n",
    "                                formatted_species_name = species_name.replace(\" \", \"_\")\n",
    "\n",
    "                                for file_name in available_file:\n",
    "                                    if formatted_species_name in file_name and model + '_dispersal.csv.xz' in file_name:\n",
    "                                        species_file = file_name\n",
    "                                        species_file2 = [x.split(\".csv\")[0] for x in species_file] \n",
    "                                        break\n",
    "                                else:\n",
    "                                    bioscen_species = None\n",
    "                                    continue\n",
    "\n",
    "                                bioscen_species = pd.read_csv(dir_species + file_name)\n",
    "\n",
    "                                available_files_iucn = formatted_species_name + \".csv\"\n",
    "                                if available_files_iucn in os.listdir(dir_habclass):\n",
    "                                    IUCN = pd.read_csv(dir_habclass + available_files_iucn)\n",
    "                                else:\n",
    "                                    continue\n",
    "\n",
    "                                lon = bioscen_species[\"x\"]\n",
    "                                lat = bioscen_species[\"y\"]\n",
    "                                z = bioscen_species[bioscen_model_name + '_' + scenario + '_' + selected_year]\n",
    "\n",
    "                                df = pd.DataFrame({\"lon\": lon, \"lat\": lat, \"vals\": z})\n",
    "                                df = df.fillna(0)\n",
    "                                convcodes_renamed = convcodes.rename(columns={'IUCN_hab':'result.code'})\n",
    "                                Habitats = IUCN.merge(convcodes_renamed, left_on='result.code', right_on='result.code')\n",
    "\n",
    "                                keys = ['LUH1', 'LUH2', 'LUH3', 'LUH4', 'LUH5', 'LUH6', 'LUH7', 'LUH8','LUH9','LUH10', 'LUH11', 'LUH12']\n",
    "                                split_cols = Habitats['LUH'].str.split('.', expand=True)\n",
    "                                for i, key in enumerate(keys):\n",
    "                                    if i < len(split_cols.columns):\n",
    "                                        Habitats[key] = split_cols[i]\n",
    "                                    else:\n",
    "                                        Habitats[key] = pd.Series(dtype='float64')\n",
    "                                if len(Habitats.columns) > len(keys) + 1:\n",
    "                                    num_missing_cols = len(Habitats.columns) - len(keys) - 1\n",
    "                                    Habitats = Habitats.reindex(columns=list(Habitats.columns) + ['LUH{}'.format(i) for i in range(13, 13 + num_missing_cols)], fill_value=np.nan)\n",
    "                                    Habitats.drop('LUH', axis=1, inplace=True)\n",
    "                                Habitats_suitable = Habitats[Habitats['result.suitability'] == 'Suitable'].copy()\n",
    "                                if k == 0 and l == 0:\n",
    "                                    LandUseList = \"/storage/workspaces/wa_climate/climate_trt/data/LUH2/\" + ssprcps_longs[k] + \"/multiple-states_input4MIPs_landState_ScenarioMIP_UofMD-IMAGE-\" + ssprcps_shorts[l] + \"-2-1-f_gn_2015-2100.nc\"\n",
    "                                elif k == 1 and l == 1:\n",
    "                                    LandUseList = \"/storage/workspaces/wa_climate/climate_trt/data/LUH2/\" + ssprcps_longs[k] + \"/multiple-states_input4MIPs_landState_ScenarioMIP_UofMD-GCAM-\" + ssprcps_shorts[l] + \"-2-1-f_gn_2015-2100.nc\"\n",
    "\n",
    "\n",
    "                                isimip = xr.open_dataarray(\"/storage/workspaces/wa_climate/climate_trt/data/ISIMIP/ISIMIP3b/InputData/GCM/global/miroc6_r1i1p1f1_w5e5_ssp585_tasmin_global_daily_2071_2080.nc\")\n",
    "\n",
    "\n",
    "                                ncfname = LandUseList\n",
    "                                da_landuse =  xarray.open_dataset(ncfname, decode_times=False)\n",
    "                                da_landuse = da_landuse.isel(time=time)\n",
    "\n",
    "                                #prifdf_bin = xr.where(prifdf > 0, 1, 0)\n",
    "                                df_sdm =df\n",
    "                                da_landuse = da_landuse.coarsen(lon=2).mean().coarsen(lat=2).mean()\n",
    "\n",
    "                                #build an empty np.array \n",
    "                                np_empty = np.zeros_like(da_landuse['primf'].values, dtype=float)\n",
    "\n",
    "                                isimip_lats = isimip['lat'].values\n",
    "                                isimip_lons = isimip['lon'].values\n",
    "\n",
    "                                da_empty = xr.DataArray(np_empty, coords=[time, isimip_lats, isimip_lons], dims=['time','lats','lons'])\n",
    "                                da_landclim = da_empty.assign_attrs(da_landuse)\n",
    "\n",
    "                                keys = [\"primn\" if row[f\"LUH{i}\"] == \"primn\" else row[f\"LUH{i}\"] for _, row in Habitats_suitable.iterrows() for i in range(1, 5) if pd.notna(row[f\"LUH{i}\"])]\n",
    "                                keys = list(set(keys))\n",
    "\n",
    "                                # Compute the product with the \"newvalue\" column and assign it to a new column in the merged DataFrame\n",
    "                                for code in keys: \n",
    "                                    # Compute the product with the \"newvalue\" column and assign it to a new column in the merged DataFrame\n",
    "                                    latitudes = df_sdm['lat'].unique()\n",
    "                                    longitudes = df_sdm['lon'].unique()\n",
    "\n",
    "                                    lats_sorted = np.sort(latitudes)\n",
    "                                    lons_sorted = np.sort(longitudes)\n",
    "\n",
    "                                    newvalue_array = np.zeros((len(lats_sorted), len(lons_sorted)))\n",
    "                                    for i, lat in enumerate(lats_sorted):\n",
    "                                        for j, lon in enumerate(lons_sorted):\n",
    "                                            selection = df_sdm[(df_sdm['lat'] == lat) & (df_sdm['lon'] == lon)]\n",
    "                                            if not selection.empty:\n",
    "                                                newvalue_array[i, j] = selection['vals'].values[0]\n",
    "\n",
    "                                    da = xr.DataArray(newvalue_array, coords=[lats_sorted, lons_sorted], dims=['lat', 'lon'])\n",
    "                                    # Interpolate the values of newvalue to the dimensions of A\n",
    "                                    interpolated_values = da.interp(lat=isimip['lat'].values, lon=isimip['lon'].values)\n",
    "\n",
    "                                    # Add the interpolated values to the A DataArray\n",
    "                                    da_landuse['newvalue'] = interpolated_values\n",
    "                                    da_landuse['newvalue'] = interpolated_values.fillna(0)\n",
    "\n",
    "                                    # Compute the product with the LUH code and the \"newvalue\" column, and assign it to a new column in the merged DataFrame\n",
    "                                    np_empty = np.zeros_like(da_landuse[code].values, dtype=float)\n",
    "                                    da_landuse[f\"{code}_bin\"] = da_landuse[code] * da_landuse[\"newvalue\"]\n",
    "\n",
    "                                    # Select the DataArrays ending in \"_bin\"\n",
    "                                    bin_arrays = [da_landuse[var] for var in da_landuse.data_vars if var.endswith(\"_bin\")]\n",
    "\n",
    "                                    # Multiply all the arrays together\n",
    "                                    sum_bin = reduce(lambda x, y: x + y, bin_arrays)\n",
    "                                    # Assign the \"product_bin\" attribute to the da_landuse DataArray\n",
    "                                    da_landuse[\"sum_bin\"] = sum_bin\n",
    "                                    difference = da_landuse[\"sum_bin\"] - da_landuse[\"newvalue\"]\n",
    "                                    da_landuse[\"difference_filter\"] = difference\n",
    "\n",
    "                                    da_landclim = da_landclim.assign_attrs(da_landuse)\n",
    "\n",
    "                                    da_landuse.to_netcdf(\"/storage/homefs/ch21o450/scripts/BioScenComb/data/LandClim_Output/\" + model+ \"/\" + taxa + \"/\" + model_name + \"/\" + scenario + \"/\" + formatted_species_name + \"_\" + str(time)+ \".nc\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
