{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35315dbf-e378-4b99-bea2-eb244e6edfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "#pip install  rioxarray==0.3.1\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import rioxarray\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import cartopy.crs as ccrs\n",
    "import rasterio\n",
    "import os\n",
    "import matplotlib.colors\n",
    "scriptsdir = os.getcwd()\n",
    "from scipy.interpolate import griddata\n",
    "from functools import reduce\n",
    "import itertools\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "from line_profiler import LineProfiler\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88154602-7463-4cbe-a314-23173189c96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "time=[35]\n",
    "models = [\"GAM\"]\n",
    "taxas=[\"Mammals\"]\n",
    "scenarios=[\"rcp26\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e03ec424-cb3c-460b-a1f6-133aee079e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "years= ['1845', '1990', '1995', '2009', '2010', '2020', '2026', '2032', '2048', '2050','2052', '2056', '2080', '2100', '2150', '2200', '2250']\n",
    "year_indices = {35: 9, 65: 12, 85: 13}\n",
    "selected_year = years[year_indices[time[0]]]\n",
    "if time[0] == 35 or time[0] == 65:\n",
    "    model_names = ['GFDL-ESM2M', 'IPSL-CM5A-LR', 'HadGEM2-ES', 'MIROC5']\n",
    "    bioscen_model_names = ['GFDL.ESM2M', 'IPSL.CM5A-LR', 'HadGEM2.ES', 'MIROC5']\n",
    "    scenarios = [\"rcp26\"]\n",
    "    ssprcps_shorts = [\"ssp126\"]\n",
    "elif time[0] == 85:\n",
    "    model_names = ['IPSL-CM5A-LR', 'HadGEM2-ES', 'MIROC5']\n",
    "    bioscen_model_names = ['IPSL.CM5A-LR', 'HadGEM2.ES', 'MIROC5']\n",
    "    scenarios = [\"rcp26\"]\n",
    "    ssprcps_shorts = [\"ssp126\"]\n",
    "    \n",
    "combinations = list(itertools.product(models, model_names))\n",
    "    # Load necessary data\n",
    "convcodes = pd.read_csv(\"/storage/homefs/ch21o450/scripts/BioScenComb/data/IUCN_LUH_converion_table_Carlson.csv\")\n",
    "\n",
    "for taxa in taxas:# Get all possible combinations of models and model_names    \n",
    "    for model in models :\n",
    "        for model_name in model_names:\n",
    "            for bioscen_model_name in bioscen_model_names:\n",
    "                for scenario in scenarios:\n",
    "                    for l, ssprcp_short in enumerate(ssprcps_shorts):\n",
    "\n",
    "                        convcodes = pd.read_csv(\"/storage/homefs/ch21o450/scripts/BioScenComb/data/IUCN_LUH_converion_table_Carlson.csv\")\n",
    "                        dir_habclass = \"/storage/homefs/ch21o450/IUCN/Habitat_Classifications/\" + taxa + \"/\"\n",
    "\n",
    "                        dir_species = \"/storage/workspaces/wa_climate/climate_trt/data/BioScen15/individual_projections/\" + taxa+ \"_\" + model +\"_results_climate/\"\n",
    "                        available_file = os.listdir(dir_species)\n",
    "                        available_names = [x.split(\".csv\")[0] for x in available_file]\n",
    "\n",
    "                        formatted_names = []\n",
    "\n",
    "                        for species_name in available_names:\n",
    "                            split_species_name = species_name.split(\"_\")[:2]\n",
    "                            formatted_species_name = \" \".join(split_species_name)\n",
    "                            formatted_names.append(formatted_species_name)\n",
    "\n",
    "                        results = []\n",
    "                        for i, species_name in enumerate(formatted_names[:1]):\n",
    "                            formatted_species_name = species_name.replace(\" \", \"_\")\n",
    "\n",
    "                            for file_name in available_file:\n",
    "                                if formatted_species_name in file_name and model + '_dispersal.csv.xz' in file_name:\n",
    "                                    species_file = file_name\n",
    "                                    species_file2 = [x.split(\".csv\")[0] for x in species_file] \n",
    "                                    break\n",
    "                            else:\n",
    "                                bioscen_species = None\n",
    "                                continue\n",
    "\n",
    "                            bioscen_species = pd.read_csv(dir_species + file_name)\n",
    "\n",
    "                            available_files_iucn = formatted_species_name + \".csv\"\n",
    "                            if available_files_iucn in os.listdir(dir_habclass):\n",
    "                                IUCN = pd.read_csv(dir_habclass + available_files_iucn)\n",
    "                            else:\n",
    "                                continue\n",
    "\n",
    "                            lon = bioscen_species[\"x\"]\n",
    "                            lat = bioscen_species[\"y\"]\n",
    "                            z = bioscen_species[bioscen_model_name + '_' + scenario + '_' + selected_year]\n",
    "\n",
    "                            df = pd.DataFrame({\"lon\": lon, \"lat\": lat, \"vals\": z})\n",
    "                            df = df.fillna(0)\n",
    "                            convcodes_renamed = convcodes.rename(columns={'IUCN_hab':'result.code'})\n",
    "                            Habitats = IUCN.merge(convcodes_renamed, left_on='result.code', right_on='result.code')\n",
    "\n",
    "                            keys = ['LUH1', 'LUH2', 'LUH3', 'LUH4', 'LUH5', 'LUH6', 'LUH7', 'LUH8','LUH9','LUH10', 'LUH11', 'LUH12']\n",
    "                            split_cols = Habitats['LUH'].str.split('.', expand=True)\n",
    "                            for i, key in enumerate(keys):\n",
    "                                if i < len(split_cols.columns):\n",
    "                                    Habitats[key] = split_cols[i]\n",
    "                                else:\n",
    "                                    Habitats[key] = pd.Series(dtype='float64')\n",
    "                            if len(Habitats.columns) > len(keys) + 1:\n",
    "                                num_missing_cols = len(Habitats.columns) - len(keys) - 1\n",
    "                                Habitats = Habitats.reindex(columns=list(Habitats.columns) + ['LUH{}'.format(i) for i in range(13, 13 + num_missing_cols)], fill_value=np.nan)\n",
    "                                Habitats.drop('LUH', axis=1, inplace=True)\n",
    "                            Habitats_suitable = Habitats[Habitats['result.suitability'] == 'Suitable'].copy()\n",
    "\n",
    "                            LandUseList = \"/storage/workspaces/wa_climate/climate_trt/chari/LUH2/remapped_luh2_\" + ssprcps_shorts[l] + \".nc\"\n",
    "\n",
    "                            #isimip = xr.open_dataarray(\"/storage/workspaces/wa_climate/climate_trt/data/ISIMIP/ISIMIP3b/InputData/GCM/global/miroc6_r1i1p1f1_w5e5_ssp585_tasmin_global_daily_2071_2080.nc\")\n",
    "\n",
    "\n",
    "                            ncfname = LandUseList\n",
    "                            da_landuse =  xr.open_dataset(ncfname, decode_times=False)\n",
    "                            da_landuse = da_landuse.isel(time=time)\n",
    "\n",
    "                            #prifdf_bin = xr.where(prifdf > 0, 1, 0)\n",
    "                            df_sdm =df\n",
    "\n",
    "                            #build an empty np.array \n",
    "                            np_empty = np.zeros_like(da_landuse['primf'].values, dtype=float)\n",
    "\n",
    "                            #isimip_lats = isimip['lat'].values\n",
    "                            #isimip_lons = isimip['lon'].values\n",
    "\n",
    "                            lats = da_landuse['lat'].values\n",
    "                            lons = da_landuse['lon'].values\n",
    "\n",
    "                            da_empty = xr.DataArray(np_empty, coords=[time, lats, lons], dims=['time','lats','lons'])\n",
    "                            da_landclim = da_empty.assign_attrs(da_landuse)\n",
    "\n",
    "                            # Compute the product with the \"newvalue\" column and assign it to a new column in the merged DataFrame\n",
    "\n",
    "                            # Compute the product with the \"newvalue\" column and assign it to a new column in the merged DataFrame\n",
    "                            latitudes = df_sdm['lat'].unique()\n",
    "                            longitudes = df_sdm['lon'].unique()\n",
    "\n",
    "                            lats_sorted = np.sort(latitudes)\n",
    "                            lons_sorted = np.sort(longitudes)\n",
    "\n",
    "                           # Create a dictionary with (lat, lon) tuples as keys and the corresponding values from df_sdm as values\n",
    "                            sdm_dict = {(lat, lon): vals for lat, lon, vals in df_sdm[['lat', 'lon', 'vals']].to_numpy()}\n",
    "\n",
    "                            # Initialize the newvalue_array with NaNs instead of zeros\n",
    "                            newvalue_array = np.full((len(lats_sorted), len(lons_sorted)), np.nan)\n",
    "\n",
    "                            # Loop over the latitudes and longitudes and use the dictionary to perform the lookup\n",
    "                            for i, lat in enumerate(lats_sorted):\n",
    "                                for j, lon in enumerate(lons_sorted):\n",
    "                                    vals = sdm_dict.get((lat, lon), np.nan)\n",
    "                                    if not np.isnan(vals):\n",
    "                                        newvalue_array[i, j] = vals\n",
    "\n",
    "\n",
    "                            da = xr.DataArray(newvalue_array, coords=[lats_sorted, lons_sorted], dims=['lat', 'lon'])\n",
    "                            # Interpolate the values of newvalue to the dimensions of A\n",
    "                            interpolated_values = da.interp(lat=lats, lon=lons)\n",
    "\n",
    "                            # Add the interpolated values to the A DataArray\n",
    "                            da_landuse['newvalue'] = interpolated_values\n",
    "                            da_landuse['newvalue'] = interpolated_values.fillna(0)\n",
    "                            \n",
    "                               \n",
    "                            keys = [row[f\"LUH{i}\"] for _, row in Habitats_suitable.iterrows() for i in range(1, 5) if pd.notna(row[f\"LUH{i}\"])]\n",
    "\n",
    "                            keys = list(set(keys))\n",
    "\n",
    "                            num_codes = 0\n",
    "                            for code in keys: \n",
    "                                # Check if the code is \"MARINE\" and skip land-use filter if it is\n",
    "                                if code == \"MARINE\":\n",
    "                                    pass\n",
    "                                else:\n",
    "                                    num_codes += 1\n",
    "                                    # Compute the product with the LUH code and the \"newvalue\" column, and assign it to a new column in the merged DataFrame\n",
    "                                    np_empty = np.zeros_like(da_landuse[code].values, dtype=float)\n",
    "                                    da_landuse[f\"{code}_bin\"] = da_landuse[code] * da_landuse[\"newvalue\"]\n",
    "                                    \n",
    "                                    da_landuse[f\"{code}_binary\"] = (da_landuse[code] > 0).astype(float)\n",
    "                                    #da_landuse[f\"{code}_lu_binary\"] = da_landuse[f\"{code}_binary\"] * da_landuse[\"newvalue\"]\n",
    "                                    \n",
    "                                    #da_landuse[\"newvalue_binary\"] = (da_landuse[\"newvalue\"] >0).astype(float)\n",
    "                                    #da_landuse[f\"{code}_poo_lu_binary\"] = da_landuse[f\"{code}_binary\"] * da_landuse[\"newvalue_binary\"]\n",
    "                                    # Select the DataArrays ending in \"_bin\"\n",
    "                                    bin_arrays = [da_landuse[var] for var in da_landuse.data_vars if var.endswith(\"_bin\") and var != \"sum_bin\"]\n",
    "                                    #binary_arrays = [da_landuse[var] for var in da_landuse.data_vars if var.endswith(\"_lu_binary\") and var != \"sum_lu_binary\"]\n",
    "                                    #binary_poo_lu_arrays = [da_landuse[var] for var in da_landuse.data_vars if var.endswith(\"_poo_lu_binary\") and var != \"sum_poo_lu_binary\"]\n",
    "\n",
    "                                    # Multiply all the arrays together\n",
    "                                    sum_bin = reduce(lambda x, y: x + y, bin_arrays)\n",
    "                                    #sum_lu_binary = reduce(lambda x, y: x + y, binary_arrays)\n",
    "                                    #sum_poo_lu_binary = reduce(lambda x, y: x + y, binary_poo_lu_arrays)\n",
    "                                    \n",
    "                                    # Assign the \"product_bin\" attribute to the da_landuse DataArray\n",
    "                                    da_landuse[\"sum_bin\"] = sum_bin\n",
    "                                    #da_landuse[\"sum_lu_binary\"] = sum_lu_binary\n",
    "                                    #da_landuse[\"sum_poo_lu_binary\"] = sum_poo_lu_binary\n",
    "                                    #da_landuse[\"sum_poo_lu_binary_norm\"] = da_landuse[\"sum_poo_lu_binary\"] / num_codes\n",
    "                                    \n",
    "                                    #difference = da_landuse[\"sum_bin\"] - da_landuse[\"newvalue\"]\n",
    "                                    #da_landuse[\"difference_filter\"] = difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f8dabb2-1f4f-4ec0-9453-90e9a814b18c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ssp460'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " ssprcps_shorts[l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "662c803f-3767-4063-a182-a595bb9599cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rcp60'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f06bd3a-8ee8-482b-a6ab-a41ee352ce82",
   "metadata": {},
   "outputs": [],
   "source": [
    "rcp26 = da_landuse[\"sum_bin\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d2a4af0-94d4-41c0-94df-bf187569bcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rcp60 = da_landuse[\"sum_bin\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f055af30-d79b-4a13-8c79-508978971f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff =rcp26 -rcp60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0395e6-61c7-432b-bbd5-271640228ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
