{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c9f26ea-3587-45e1-a19b-6514d084b8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "#pip install  rioxarray==0.3.1\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import rioxarray\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import cartopy.crs as ccrs\n",
    "import rasterio\n",
    "import os\n",
    "import matplotlib.colors\n",
    "scriptsdir = os.getcwd()\n",
    "from scipy.interpolate import griddata\n",
    "from functools import reduce\n",
    "import xarray\n",
    "import itertools\n",
    "import argparse\n",
    "import matplotlib.colors as mcolors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c558433-27e6-4426-b7ae-153c438c2fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "models=[\"GAM\",\"GBM\"]\n",
    "taxas=[\"Mammals\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56397477-8173-4024-a042-749cd1bfdd76",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/scratch/local/61799221/ipykernel_22248/2369097233.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'ds' is not defined"
     ]
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f7a31270-54e1-487d-8e7c-4ee721e4afb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "time=[1146]\n",
    "years= ['1845', '1990', '1995', '2009', '2010', '2020', '2026', '2032', '2048', '2050','2052', '2056', '2080', '2100', '2150', '2200', '2250']\n",
    "year_indices = {1146:2,35: 9, 65: 12, 85: 13}\n",
    "selected_year = years[year_indices[time[0]]]\n",
    "if time[0] == 1146:\n",
    "    model_names = ['EWEMBI']\n",
    "    bioscen_model_names = ['EWEMBI']\n",
    "       \n",
    "combinations = list(itertools.product(models, model_names))\n",
    "    # Load necessary data\n",
    "convcodes = pd.read_csv(\"/storage/homefs/ch21o450/scripts/BioScenComb/data/IUCN_LUH_converion_table_Carlson.csv\")\n",
    "\n",
    "\n",
    "for taxa in taxas:# Get all possible combinations of models and model_names    \n",
    "    for model in models :\n",
    "        for model_name in model_names:\n",
    "            for bioscen_model_name in bioscen_model_names:\n",
    "\n",
    "                convcodes = pd.read_csv(\"/storage/homefs/ch21o450/scripts/BioScenComb/data/IUCN_LUH_converion_table_Carlson.csv\")\n",
    "                dir_habclass = \"/storage/homefs/ch21o450/IUCN/Habitat_Classifications/\" + taxa + \"/\"\n",
    "\n",
    "                dir_species = \"/storage/workspaces/wa_climate/climate_trt/data/BioScen15/individual_projections/\" + taxa+ \"_\" + model +\"_results_climate/\"\n",
    "                available_file = os.listdir(dir_species)\n",
    "\n",
    "                results = []\n",
    "                for i, species_name in enumerate(formatted_names):\n",
    "                    formatted_species_name = formatted_species_name =  \"Crocidura_olivieri\"\n",
    "\n",
    "                    for file_name in available_file:\n",
    "                        if formatted_species_name in file_name and model + '_dispersal.csv.xz' in file_name:\n",
    "                            species_file = file_name\n",
    "                            species_file2 = [x.split(\".csv\")[0] for x in species_file] \n",
    "                            break\n",
    "                    else:\n",
    "                        bioscen_species = None\n",
    "                        continue\n",
    "\n",
    "                    bioscen_species = pd.read_csv(dir_species + file_name)\n",
    "\n",
    "                    available_files_iucn = formatted_species_name + \".csv\"\n",
    "                    if available_files_iucn in os.listdir(dir_habclass):\n",
    "                        IUCN = pd.read_csv(dir_habclass + available_files_iucn)\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "                    lon = bioscen_species[\"x\"]\n",
    "                    lat = bioscen_species[\"y\"]\n",
    "                    z = bioscen_species[bioscen_model_name + '_' + selected_year]\n",
    "\n",
    "                    df = pd.DataFrame({\"lon\": lon, \"lat\": lat, \"vals\": z})\n",
    "                    df = df.fillna(0)\n",
    "                    convcodes_renamed = convcodes.rename(columns={'IUCN_hab':'result.code'})\n",
    "                    IUCN['result.code'] = pd.to_numeric(IUCN['result.code'], errors='coerce')\n",
    "                    Habitats = IUCN.merge(convcodes_renamed, left_on='result.code', right_on='result.code')\n",
    "\n",
    "                    keys = ['LUH1', 'LUH2', 'LUH3', 'LUH4', 'LUH5', 'LUH6', 'LUH7', 'LUH8','LUH9','LUH10', 'LUH11', 'LUH12']\n",
    "                    split_cols = Habitats['LUH'].str.split('.', expand=True)\n",
    "                    for i, key in enumerate(keys):\n",
    "                        if i < len(split_cols.columns):\n",
    "                            Habitats[key] = split_cols[i]\n",
    "                        else:\n",
    "                            Habitats[key] = pd.Series(dtype='float64')\n",
    "                    if len(Habitats.columns) > len(keys) + 1:\n",
    "                        num_missing_cols = len(Habitats.columns) - len(keys) - 1\n",
    "                        Habitats = Habitats.reindex(columns=list(Habitats.columns) + ['LUH{}'.format(i) for i in range(13, 13 + num_missing_cols)], fill_value=np.nan)\n",
    "                        Habitats.drop('LUH', axis=1, inplace=True)\n",
    "                    Habitats_suitable = Habitats[Habitats['result.suitability'] == 'Suitable'].copy()\n",
    "\n",
    "                    LandUseList = \"/storage/workspaces/wa_climate/climate_trt/chari/LUH2/remapped_luh2_historical.nc\"\n",
    "\n",
    "                    #isimip = xr.open_dataarray(\"/storage/workspaces/wa_climate/climate_trt/data/ISIMIP/ISIMIP3b/InputData/GCM/global/miroc6_r1i1p1f1_w5e5_ssp585_tasmin_global_daily_2071_2080.nc\")\n",
    "\n",
    "\n",
    "                    ncfname = LandUseList\n",
    "                    da_landuse =  xr.open_dataset(ncfname, decode_times=False)\n",
    "                    da_landuse = da_landuse.isel(time=time)\n",
    "\n",
    "                    #prifdf_bin = xr.where(prifdf > 0, 1, 0)\n",
    "                    df_sdm =df\n",
    "\n",
    "                    #build an empty np.array \n",
    "                    np_empty = np.zeros_like(da_landuse['primf'].values, dtype=float)\n",
    "\n",
    "                    #isimip_lats = isimip['lat'].values\n",
    "                    #isimip_lons = isimip['lon'].values\n",
    "\n",
    "                    lats = da_landuse['lat'].values\n",
    "                    lons = da_landuse['lon'].values\n",
    "\n",
    "                    da_empty = xr.DataArray(np_empty, coords=[time, lats, lons], dims=['time','lats','lons'])\n",
    "                    da_landclim = da_empty.assign_attrs(da_landuse)\n",
    "\n",
    "                    # Compute the product with the \"newvalue\" column and assign it to a new column in the merged DataFrame\n",
    "\n",
    "                    # Compute the product with the \"newvalue\" column and assign it to a new column in the merged DataFrame\n",
    "                    latitudes = df_sdm['lat'].unique()\n",
    "                    longitudes = df_sdm['lon'].unique()\n",
    "\n",
    "                    lats_sorted = np.sort(latitudes)\n",
    "                    lons_sorted = np.sort(longitudes)\n",
    "\n",
    "                    # Create a dictionary with (lat, lon) tuples as keys and the corresponding values from df_sdm as values\n",
    "                    sdm_dict = {(lat, lon): vals for lat, lon, vals in df_sdm[['lat', 'lon', 'vals']].to_numpy()}\n",
    "\n",
    "                    # Initialize the newvalue_array with NaNs instead of zeros\n",
    "                    newvalue_array = np.full((len(lats_sorted), len(lons_sorted)), np.nan)\n",
    "\n",
    "                    # Loop over the latitudes and longitudes and use the dictionary to perform the lookup\n",
    "                    for i, lat in enumerate(lats_sorted):\n",
    "                        for j, lon in enumerate(lons_sorted):\n",
    "                            vals = sdm_dict.get((lat, lon), np.nan)\n",
    "                            if not np.isnan(vals):\n",
    "                                newvalue_array[i, j] = vals\n",
    "\n",
    "\n",
    "                    da = xr.DataArray(newvalue_array, coords=[lats_sorted, lons_sorted], dims=['lat', 'lon'])\n",
    "                    # Interpolate the values of newvalue to the dimensions of A\n",
    "                    interpolated_values = da.interp(lat=lats, lon=lons)\n",
    "\n",
    "                    # Add the interpolated values to the A DataArray\n",
    "                    da_landuse['newvalue'] = interpolated_values\n",
    "                    da_landuse['newvalue'] = interpolated_values.fillna(0)\n",
    "                    \n",
    "                    \n",
    "   \n",
    "                    keys = [row[f\"LUH{i}\"] for _, row in Habitats_suitable.iterrows() for i in range(1, 5) if pd.notna(row[f\"LUH{i}\"])]\n",
    "\n",
    "                    keys = list(set(keys))\n",
    "\n",
    "                    for variable in list(da_landuse.keys()):\n",
    "                        if variable not in keys and variable != \"newvalue\":\n",
    "                            del da_landuse[variable]\n",
    "\n",
    "                    num_codes = 0\n",
    "                    for code in keys: \n",
    "                        # Check if the code is \"MARINE\" and skip land-use filter if it is\n",
    "                        if code == \"MARINE\" or not keys:\n",
    "                            pass\n",
    "                        else:\n",
    "                            num_codes += 1\n",
    "                            # Compute the product with the LUH code and the \"newvalue\" column, and assign it to a new column in the merged DataFrame\n",
    "                            np_empty = np.zeros_like(da_landuse[code].values, dtype=float)\n",
    "                            da_landuse[f\"{code}_bin\"] = da_landuse[code] * da_landuse[\"newvalue\"]\n",
    "\n",
    "                            da_landuse[f\"{code}_binary\"] = (da_landuse[code] > 0).astype(float)\n",
    "                            da_landuse[f\"{code}_lu_binary\"] = da_landuse[f\"{code}_binary\"] * da_landuse[\"newvalue\"]\n",
    "\n",
    "                            #da_landuse[\"newvalue_binary\"] = (da_landuse[\"newvalue\"] >0).astype(float)\n",
    "                            #da_landuse[f\"{code}_poo_lu_binary\"] = da_landuse[f\"{code}_binary\"] * da_landuse[\"newvalue_binary\"]\n",
    "                            # Select the DataArrays ending in \"_bin\"\n",
    "                            bin_arrays = [da_landuse[var] for var in da_landuse.data_vars if var.endswith(\"_bin\") and var != \"sum_bin\"]\n",
    "                            binary_arrays = [da_landuse[var] for var in da_landuse.data_vars if var.endswith(\"_lu_binary\") and var != \"sum_lu_binary\"]\n",
    "                            #binary_poo_lu_arrays = [da_landuse[var] for var in da_landuse.data_vars if var.endswith(\"_poo_lu_binary\") and var != \"sum_poo_lu_binary\"]\n",
    "\n",
    "                            # Multiply all the arrays together\n",
    "                            sum_bin = reduce(lambda x, y: x + y, bin_arrays)\n",
    "                            sum_lu_binary = reduce(lambda x, y: x + y, binary_arrays)\n",
    "                            #sum_poo_lu_binary = reduce(lambda x, y: x + y, binary_poo_lu_arrays)\n",
    "\n",
    "                            # Assign the \"product_bin\" attribute to the da_landuse DataArray\n",
    "                            da_landuse[\"sum_bin\"] = sum_bin\n",
    "                            da_landuse[\"sum_lu_binary\"] = sum_lu_binary\n",
    "                            #da_landuse[\"sum_poo_lu_binary\"] = sum_poo_lu_binary\n",
    "                            #da_landuse[\"sum_poo_lu_binary_norm\"] = da_landuse[\"sum_poo_lu_binary\"] / num_codes\n",
    "\n",
    "                    for variable in list(da_landuse.keys()):\n",
    "                        if variable != \"newvalue\" and variable != \"sum_bin\" and variable != \"sum_lu_binary\":\n",
    "                            del da_landuse[variable]\n",
    "                    for code in keys: \n",
    "                        # Check if the code is \"MARINE\" and skip land-use filter if it is\n",
    "                        if code == \"MARINE\" or not keys:\n",
    "                            pass\n",
    "                        else:\n",
    "\n",
    "\n",
    "                            output_directory = \"/storage/scratch/users/ch21o450/data/LandClim_Output/\" + model+ \"/\" + taxa + \"/\" + model_name + \"/\"\n",
    "                            os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "\n",
    "                            da_landuse.to_netcdf(\"/storage/scratch/users/ch21o450/data/LandClim_Output/\" + model+ \"/\" + taxa + \"/\" + model_name + \"/\"  + formatted_species_name + \"_\" + str(time)+ \".nc\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "40cdd76d-568d-4284-a6c9-fb0f4a0d2e40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GBM'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6ee6e9da-e932-492c-bcbd-ce3d100c2dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"lon\": lon, \"lat\": lat, \"vals\": z})\n",
    "df = df.fillna(0)\n",
    "convcodes_renamed = convcodes.rename(columns={'IUCN_hab':'result.code'})\n",
    "IUCN['result.code'] = pd.to_numeric(IUCN['result.code'], errors='coerce')\n",
    "Habitats = IUCN.merge(convcodes_renamed, left_on='result.code', right_on='result.code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "82f1c587-504f-47c8-99b6-7950e2a9397a",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = ['LUH1', 'LUH2', 'LUH3', 'LUH4', 'LUH5', 'LUH6', 'LUH7', 'LUH8','LUH9','LUH10', 'LUH11', 'LUH12']\n",
    "split_cols = Habitats['LUH'].str.split('.', expand=True)\n",
    "for i, key in enumerate(keys):\n",
    "    if i < len(split_cols.columns):\n",
    "        Habitats[key] = split_cols[i]\n",
    "    else:\n",
    "        Habitats[key] = pd.Series(dtype='float64')\n",
    "if len(Habitats.columns) > len(keys) + 1:\n",
    "    num_missing_cols = len(Habitats.columns) - len(keys) - 1\n",
    "    Habitats = Habitats.reindex(columns=list(Habitats.columns) + ['LUH{}'.format(i) for i in range(13, 13 + num_missing_cols)], fill_value=np.nan)\n",
    "    Habitats.drop('LUH', axis=1, inplace=True)\n",
    "Habitats_suitable = Habitats[Habitats['result.suitability'] == 'Suitable'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e71706fb-2c9d-40f0-9cb7-f8742f4c5b27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|    |   Unnamed: 0 | name                  | region_identifier   |   result.code | result.habitat                                        | result.suitability   |   result.season | result.majorimportance   |   LUH1 |   LUH2 |   LUH3 |   LUH4 |   LUH5 |   LUH6 |   LUH7 |   LUH8 |   LUH9 |   LUH10 |   LUH11 |   LUH12 |   LUH13 |   LUH14 |   LUH15 |   LUH16 |   LUH17 |   LUH18 |   LUH19 |   LUH20 |\\n|---:|-------------:|:----------------------|:--------------------|--------------:|:------------------------------------------------------|:---------------------|----------------:|:-------------------------|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|-------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|\\n|  0 |            1 | Chilonatalus micropus | global              |           7.1 | Caves and Subterranean Habitats (non-aquatic) - Caves | Suitable             |             nan | Yes                      |    nan |    nan |    nan |    nan |    nan |    nan |    nan |    nan |    nan |     nan |     nan |     nan |     nan |     nan |     nan |     nan |     nan |     nan |     nan |     nan |'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Habitats_suitable.to_markdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4cc1516-b826-4511-b152-b56d2a1a1dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "                    \n",
    "   \n",
    "                    keys = [row[f\"LUH{i}\"] for _, row in Habitats_suitable.iterrows() for i in range(1, 5) if pd.notna(row[f\"LUH{i}\"])]\n",
    "\n",
    "                    keys = list(set(keys))\n",
    "\n",
    "                    for variable in list(da_landuse.keys()):\n",
    "                        if variable not in keys and variable != \"newvalue\":\n",
    "                            del da_landuse[variable]\n",
    "\n",
    "                    num_codes = 0\n",
    "                    for code in keys: \n",
    "                        # Check if the code is \"MARINE\" and skip land-use filter if it is\n",
    "                        if code == \"MARINE\" or not keys:\n",
    "                            pass\n",
    "                        else:\n",
    "                            num_codes += 1\n",
    "                            # Compute the product with the LUH code and the \"newvalue\" column, and assign it to a new column in the merged DataFrame\n",
    "                            np_empty = np.zeros_like(da_landuse[code].values, dtype=float)\n",
    "                            da_landuse[f\"{code}_bin\"] = da_landuse[code] * da_landuse[\"newvalue\"]\n",
    "\n",
    "                            da_landuse[f\"{code}_binary\"] = (da_landuse[code] > 0).astype(float)\n",
    "                            da_landuse[f\"{code}_lu_binary\"] = da_landuse[f\"{code}_binary\"] * da_landuse[\"newvalue\"]\n",
    "\n",
    "                            #da_landuse[\"newvalue_binary\"] = (da_landuse[\"newvalue\"] >0).astype(float)\n",
    "                            #da_landuse[f\"{code}_poo_lu_binary\"] = da_landuse[f\"{code}_binary\"] * da_landuse[\"newvalue_binary\"]\n",
    "                            # Select the DataArrays ending in \"_bin\"\n",
    "                            bin_arrays = [da_landuse[var] for var in da_landuse.data_vars if var.endswith(\"_bin\") and var != \"sum_bin\"]\n",
    "                            binary_arrays = [da_landuse[var] for var in da_landuse.data_vars if var.endswith(\"_lu_binary\") and var != \"sum_lu_binary\"]\n",
    "                            #binary_poo_lu_arrays = [da_landuse[var] for var in da_landuse.data_vars if var.endswith(\"_poo_lu_binary\") and var != \"sum_poo_lu_binary\"]\n",
    "\n",
    "                            # Multiply all the arrays together\n",
    "                            sum_bin = reduce(lambda x, y: x + y, bin_arrays)\n",
    "                            sum_lu_binary = reduce(lambda x, y: x + y, binary_arrays)\n",
    "                            #sum_poo_lu_binary = reduce(lambda x, y: x + y, binary_poo_lu_arrays)\n",
    "\n",
    "                            # Assign the \"product_bin\" attribute to the da_landuse DataArray\n",
    "                            da_landuse[\"sum_bin\"] = sum_bin\n",
    "                            da_landuse[\"sum_lu_binary\"] = sum_lu_binary\n",
    "                            #da_landuse[\"sum_poo_lu_binary\"] = sum_poo_lu_binary\n",
    "                            #da_landuse[\"sum_poo_lu_binary_norm\"] = da_landuse[\"sum_poo_lu_binary\"] / num_codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2fc7c0ce-b922-4754-83fe-de9c7d42ab79",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [row[f\"LUH{i}\"] for _, row in Habitats_suitable.iterrows() for i in range(1, 5) if pd.notna(row[f\"LUH{i}\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b910f122-e685-4827-bfc7-7963d5210702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa77ef58-cbdd-476e-a38c-90f494dd8ffd",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: b'/storage/scratch/users/ch21o450/data/LandClim_Output/GBM/Mammals/GFDL-ESM2M/rcp26/Chilonatalus_micropus_[35].nc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/xarray/backends/file_manager.py\u001b[0m in \u001b[0;36m_acquire_with_cache_info\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m                 \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/xarray/backends/lru_cache.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove_to_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: [<class 'netCDF4._netCDF4.Dataset'>, ('/storage/scratch/users/ch21o450/data/LandClim_Output/GBM/Mammals/GFDL-ESM2M/rcp26/Chilonatalus_micropus_[35].nc',), 'r', (('clobber', True), ('diskless', False), ('format', 'NETCDF4'), ('persist', False)), '93d9b049-e503-4156-968e-5fb43129349b']",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/scratch/local/61799221/ipykernel_22248/2415711795.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0mscenario\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"rcp26\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0mmean_value_bin_future\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnewvalue_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuture_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetcdf_path_format_future\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_historical\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscenario\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscenario\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0mmean_sum_bin_future\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuture_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetcdf_path_format_future\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_historical\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscenario\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscenario\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0mmean_value_bin_hist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnewvalue_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistorical_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetcdf_path_format_hist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_historical\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/local/61799221/ipykernel_22248/2415711795.py\u001b[0m in \u001b[0;36mnewvalue_fun\u001b[0;34m(time, model, netcdf_path_format, is_historical, scenario)\u001b[0m\n\u001b[1;32m     20\u001b[0m                     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetcdf_path_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtaxa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspecies_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetcdf_path_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtaxa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscenario\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspecies_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0mnewvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"newvalue\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/xarray/backends/api.py\u001b[0m in \u001b[0;36mopen_dataset\u001b[0;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, backend_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m     \u001b[0moverwrite_encoded_chunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"overwrite_encoded_chunks\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m     backend_ds = backend.open_dataset(\n\u001b[0m\u001b[1;32m    527\u001b[0m         \u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0mdrop_variables\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop_variables\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/xarray/backends/netCDF4_.py\u001b[0m in \u001b[0;36mopen_dataset\u001b[0;34m(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta, group, mode, format, clobber, diskless, persist, lock, autoclose)\u001b[0m\n\u001b[1;32m    575\u001b[0m     ):\n\u001b[1;32m    576\u001b[0m         \u001b[0mfilename_or_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_normalize_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m         store = NetCDF4DataStore.open(\n\u001b[0m\u001b[1;32m    578\u001b[0m             \u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m             \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/xarray/backends/netCDF4_.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(cls, filename, mode, format, group, clobber, diskless, persist, lock, lock_maker, autoclose)\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0mnetCDF4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         )\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmanager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautoclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mautoclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_acquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneeds_lock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/xarray/backends/netCDF4_.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, manager, group, mode, lock, autoclose)\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_group\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_remote\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_remote_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/xarray/backends/netCDF4_.py\u001b[0m in \u001b[0;36mds\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    389\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_acquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mopen_store_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/xarray/backends/netCDF4_.py\u001b[0m in \u001b[0;36m_acquire\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_acquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneeds_lock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneeds_lock\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m             \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_nc4_require_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_group\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/software.el7/software/Anaconda3/2021.11-foss-2021a/lib/python3.9/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/xarray/backends/file_manager.py\u001b[0m in \u001b[0;36macquire_context\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0macquire_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneeds_lock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;34m\"\"\"Context manager for acquiring a file.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_acquire_with_cache_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneeds_lock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/xarray/backends/file_manager.py\u001b[0m in \u001b[0;36m_acquire_with_cache_info\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"mode\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                 \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                     \u001b[0;31m# ensure file doesn't get overridden when opened again\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32msrc/netCDF4/_netCDF4.pyx\u001b[0m in \u001b[0;36mnetCDF4._netCDF4.Dataset.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msrc/netCDF4/_netCDF4.pyx\u001b[0m in \u001b[0;36mnetCDF4._netCDF4._ensure_nc_success\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: b'/storage/scratch/users/ch21o450/data/LandClim_Output/GBM/Mammals/GFDL-ESM2M/rcp26/Chilonatalus_micropus_[35].nc'"
     ]
    }
   ],
   "source": [
    "\n",
    "model_names = ['GFDL-ESM2M', 'IPSL-CM5A-LR', 'HadGEM2-ES', 'MIROC5']\n",
    "years = ['1845', '1990', '1995', '2009', '2010', '2020', '2026', '2032', '2048', '2050',\n",
    "         '2052', '2056', '2080', '2100', '2150', '2200', '2250']\n",
    "\n",
    "for taxa in taxas:\n",
    "    for model in models:\n",
    "        dir_species = \"/storage/scratch/users/ch21o450/data/LandClim_Output/\" + model + \"/\" + taxa + \"/EWEMBI/\"\n",
    "        available_file = os.listdir(dir_species)\n",
    "        available_names = [x.split(\"_[1146].nc\")[0] for x in available_file]\n",
    "\n",
    "    species_names = available_names\n",
    "\n",
    "    def newvalue_fun(time, model, netcdf_path_format, is_historical=False, scenario=None):\n",
    "        newvalue_dict = {model_name: {} for model_name in model_names}\n",
    "        sum_bin_dict = {model_name: {} for model_name in model_names}\n",
    "\n",
    "        for model_name in model_names:\n",
    "            for species_name in species_names:\n",
    "                if is_historical:\n",
    "                    ds = xr.open_dataset(netcdf_path_format.format(model, taxa, species_name, time), decode_times=False)\n",
    "                else:\n",
    "                    ds = xr.open_dataset(netcdf_path_format.format(model, taxa, model_name, scenario, species_name, time), decode_times=False)\n",
    "\n",
    "                newvalue = ds[\"newvalue\"]\n",
    "                sum_bin = ds[\"sum_bin\"]\n",
    "\n",
    "                newvalue_dict[model_name][species_name] = newvalue\n",
    "                sum_bin_dict[model_name][species_name] = sum_bin\n",
    "\n",
    "        projections_dict = {}\n",
    "\n",
    "        for species_name in species_names:\n",
    "            value_list = []\n",
    "            for model_name in model_names:\n",
    "                value_bin = newvalue_dict[model_name][species_name]\n",
    "                #value_bin = value_bin.where(value_bin > 0, 1)\n",
    "                #value_bin = (value_bin > 0.00)\n",
    "\n",
    "                value_list.append(value_bin)\n",
    "            value_bin_concat = xr.concat(value_list, dim=\"model_name\")\n",
    "            mean_value_bin = value_bin_concat.mean(dim=\"model_name\")\n",
    "            projections_dict[species_name] = mean_value_bin\n",
    "\n",
    "        value_bin_list = list(projections_dict.values())\n",
    "        mean_value_bin = xr.concat(value_bin_list, dim=\"species\").sum(dim=\"species\")  # Ensemble mean over species\n",
    "        mean_value_bin = mean_value_bin.where(mean_value_bin > 0, 0)\n",
    "        return mean_value_bin\n",
    "\n",
    "    def calculate_mean(time, model, netcdf_path_format, is_historical=False, scenario=None):\n",
    "        newvalue_dict = {model_name: {} for model_name in model_names}\n",
    "        sum_bin_dict = {model_name: {} for model_name in model_names}\n",
    "        lu_sum_bin_dict = {model_name: {} for model_name in model_names}\n",
    "\n",
    "        for model_name in model_names:\n",
    "            for species_name in species_names:\n",
    "                if is_historical:\n",
    "                    ds = xr.open_dataset(netcdf_path_format.format(model, taxa, species_name, time), decode_times=False)\n",
    "                else:\n",
    "                    ds = xr.open_dataset(netcdf_path_format.format(model, taxa, model_name, scenario, species_name, time), decode_times=False)\n",
    "                sum_bin = ds[\"sum_bin\"]\n",
    "                #lu_sum_bin = ds[\"sum_lu_binary\"]\n",
    "                #sum_bin = (sum_bin > 0.00)\n",
    "\n",
    "                sum_bin_dict[model_name][species_name] = sum_bin\n",
    "                #lu_sum_bin_dict[model_name][species_name] = lu_sum_bin\n",
    "\n",
    "        projections_dict = {}\n",
    "\n",
    "        for species_name in species_names:\n",
    "            sum_bin_list = []\n",
    "            for model_name in model_names:\n",
    "                sum_bin = sum_bin_dict[model_name][species_name]\n",
    "                sum_bin_list.append(sum_bin)\n",
    "            sum_bin_concat = xr.concat(sum_bin_list, dim=\"model_name\")\n",
    "            mean_sum_bin = sum_bin_concat.mean(dim=\"model_name\")\n",
    "            projections_dict[species_name] = mean_sum_bin\n",
    "\n",
    "        mean_sum_bin_list = list(projections_dict.values())\n",
    "        mean_sum_bin = xr.concat(mean_sum_bin_list, dim=\"species\").sum(dim=\"species\")  # Ensemble mean over species\n",
    "        mean_sum_bin = mean_sum_bin.where(mean_sum_bin > 0, 0)\n",
    "\n",
    "        return mean_sum_bin\n",
    "\n",
    "    historical_time = 1146\n",
    "    future_times = [35, 65]\n",
    "    scenario = [\"rcp26\"]\n",
    "\n",
    "    netcdf_path_format_future = \"/storage/scratch/users/ch21o450/data/LandClim_Output/{}/{}/{}/{}/{}_[{}].nc\"\n",
    "    netcdf_path_format_hist = \"/storage/scratch/users/ch21o450/data/LandClim_Output/{}/{}/EWEMBI/{}_[{}].nc\"\n",
    "    \n",
    "    future_time=35\n",
    "    scenario = \"rcp26\"\n",
    "    \n",
    "    mean_value_bin_future = newvalue_fun(future_time, model, netcdf_path_format_future, is_historical=False, scenario=scenario)\n",
    "    mean_sum_bin_future = calculate_mean(future_time, model, netcdf_path_format_future, is_historical=False, scenario=scenario)\n",
    "    mean_value_bin_hist = newvalue_fun(historical_time, model, netcdf_path_format_hist, is_historical=True)\n",
    "    mean_sum_bin_hist = calculate_mean(historical_time, model, netcdf_path_format_hist, is_historical=True)\n",
    "\n",
    "    mean_sum_bin_hist = mean_sum_bin_hist.isel(time=0)\n",
    "\n",
    "    year_indices = {1146: '1995', 35: '2050', 65: '2080', 85: '2100'}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
